import os
import re
import requests
import streamlit as st
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI
import faiss
import numpy as np
import tiktoken
from googleapiclient.discovery import build

# --- í™˜ê²½ë³€ìˆ˜ ë¡œë”© ---
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")

# --- êµ¬ê¸€ ê²€ìƒ‰ í•¨ìˆ˜ ---
def search_google(query: str, num_results: int = 3):
    service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
    res = service.cse().list(q=query, cx=GOOGLE_CSE_ID, num=num_results).execute()
    return [item["link"] for item in res.get("items", [])]

# --- URLì—ì„œ div.subXX_XX_XX í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
def crawl_url(url: str):
    try:
        html = requests.get(url, timeout=10).text
        soup = BeautifulSoup(html, "html.parser")

        match = re.search(r"sub\d{2}_\d{2}_\d{2}", url)
        if not match:
            return "âŒ URLì—ì„œ 'subXX_XX_XX' íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        class_name = match.group()
        target_div = soup.find("div", class_=class_name)

        if target_div:
            paragraphs = [p.get_text(strip=True) for p in target_div.find_all("p")]
            return "\n".join(paragraphs) if paragraphs else target_div.get_text(strip=True)

        return "âŒ í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ <div>ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        return f"âš ï¸ í¬ë¡¤ë§ ì—ëŸ¬: {str(e)}"

# --- í…ìŠ¤íŠ¸ ë¶„í•  ---
def split_text(text: str, max_tokens: int = 300):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    words = text.split()
    chunks, chunk, tokens = [], [], 0

    for word in words:
        token_len = len(tokenizer.encode(word))
        if tokens + token_len > max_tokens:
            chunks.append(" ".join(chunk))
            chunk, tokens = [], 0
        chunk.append(word)
        tokens += token_len

    if chunk:
        chunks.append(" ".join(chunk))
    return chunks

# --- ì„ë² ë”© ---
def embed_texts(texts):
    response = client.embeddings.create(
        input=texts,
        model="text-embedding-3-small"
    )
    return [d.embedding for d in response.data]

# --- FAISS ì¸ë±ìŠ¤ ---
def build_index(embeddings):
    dim = len(embeddings[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings).astype("float32"))
    return index

def search_index(index, query_embedding, texts, top_k=3):
    D, I = index.search(np.array([query_embedding]).astype("float32"), top_k)
    return [texts[i] for i in I[0]]

# --- GPT ì‘ë‹µ ---
def answer_with_context(question, context_chunks):
    context = "\n\n".join(context_chunks)
    system_msg = "ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:\n" + context
    completion = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": question}
        ]
    )
    return completion.choices[0].message.content

# --- Streamlit UI ---
st.set_page_config(page_title="ğŸ” ê²€ìƒ‰ ê¸°ë°˜ GPT ì±—ë´‡", layout="wide")
st.title("ğŸ” ê²€ìƒ‰ì–´ ê¸°ë°˜ ì›¹í¬ë¡¤ë§ + GPT ì±—ë´‡")

query = st.text_input("ğŸ’¡ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
question = st.text_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if query and question:
    with st.spinner("ğŸ” êµ¬ê¸€ì—ì„œ ê´€ë ¨ ì›¹í˜ì´ì§€ ê²€ìƒ‰ ì¤‘..."):
        urls = search_google(query, num_results=3)
        if not urls:
            st.error("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
        st.write("ğŸ”— ê²€ìƒ‰ëœ URL:")
        for i, u in enumerate(urls):
            st.markdown(f"- {i+1}. [{u}]({u})")

    selected_url = urls[0]  # ê°€ì¥ ì²« ë²ˆì§¸ URL ìë™ ì„ íƒ

    with st.spinner("ğŸŒ URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
        extracted_text = crawl_url(selected_url)
        if "âŒ" in extracted_text or "âš ï¸" in extracted_text:
            st.error(extracted_text)
            st.stop()
        st.text_area("ğŸ“„ ë³¸ë¬¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", extracted_text[:3000], height=200)

    with st.spinner("ğŸ§© í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘..."):
        chunks = split_text(extracted_text)
        if not chunks:
            st.error("âŒ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            st.stop()
        embeddings = embed_texts(chunks)
        index = build_index(embeddings)
        query_embedding = embed_texts([question])[0]
        top_chunks = search_index(index, query_embedding, chunks)

    with st.spinner("ğŸ¤– GPTê°€ ë‹µë³€ì„ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
        answer = answer_with_context(question, top_chunks)
        st.markdown("### ğŸ“¢ GPT ë‹µë³€:")
        st.write(answer)
